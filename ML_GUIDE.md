# ğŸ§  AI/ML Guide - LicitaReview

## ğŸ“š Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura AI/ML](#arquitetura-aiml)
3. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
4. [A/B Testing de Modelos](#ab-testing-de-modelos)
5. [Analytics e MÃ©tricas](#analytics-e-mÃ©tricas)
6. [User Feedback Loop](#user-feedback-loop)
7. [Best Practices](#best-practices)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ VisÃ£o Geral

O LicitaReview utiliza tÃ©cnicas avanÃ§adas de AI/ML para anÃ¡lise inteligente de editais de licitaÃ§Ã£o:

- **Modelo Principal**: Google Gemini 2.0 Flash
- **RAG Engine**: Vertex AI RAG para contexto especÃ­fico
- **A/B Testing**: ExperimentaÃ§Ã£o contÃ­nua de modelos
- **Feedback Loop**: Melhoria contÃ­nua baseada em feedback real

### Stack AI/ML

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (Next.js)                  â”‚
â”‚              User Feedback UI Components                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Layer (FastAPI)                   â”‚
â”‚  /api/experiments  â”‚  /api/analytics  â”‚  /api/feedback  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ML Core Components                    â”‚
â”‚  ABTestManager  â”‚  RAGEnhancements  â”‚  MetricsStore    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Google Cloud AI Services                â”‚
â”‚  Vertex AI RAG  â”‚  Gemini Models  â”‚  Embeddings API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Arquitetura AI/ML

### Componentes Principais

#### 1. RAG Engine

**LocalizaÃ§Ã£o**: `services/analyzer/src/services/rag_service.py`

```python
from src.services.rag_service import RAGService

# Inicializar RAG
rag = RAGService(project_id="your-project", location="us-central1")

# Criar corpus para organizaÃ§Ã£o
corpus_id = await rag.create_corpus(
    organization_id="org-123",
    display_name="Editais MinistÃ©rio X"
)

# Importar documentos
await rag.import_documents(
    corpus_id=corpus_id,
    documents=document_list
)

# Query
response = await rag.query(
    corpus_id=corpus_id,
    query="Qual o prazo de entrega?"
)
```

#### 2. A/B Testing Manager

**LocalizaÃ§Ã£o**: `services/analyzer/src/ml/ab_testing.py`

```python
from src.ml.ab_testing import ABTestManager, ModelVariant

# Criar manager
manager = ABTestManager()

# Criar experimento
experiment = manager.create_experiment(
    experiment_id="exp-gemini-pro-test",
    name="Test Gemini Pro",
    description="Testing Gemini Pro vs Flash",
    model_variant=ModelVariant.GEMINI_PRO,
    temperature=0.15,
    traffic_percentage=0.3,  # 30% do trÃ¡fego
)

# Selecionar variante para usuÃ¡rio
variant = manager.select_variant(user_id="user-123")

# Registrar resultado
manager.record_result(
    experiment_id=variant.experiment_id,
    latency_ms=1250.5,
    tokens_used=850,
    success=True
)

# Registrar feedback
manager.record_feedback(
    experiment_id=variant.experiment_id,
    is_positive=True,
    rating=4.5
)

# Comparar experimentos
comparison = manager.compare_experiments(
    "control-gemini-flash",
    "exp-gemini-pro-test"
)
print(f"Winner: {comparison['comparison']['winner']}")
```

#### 3. RAG Enhancements

**LocalizaÃ§Ã£o**: `services/analyzer/src/ml/rag_enhancements.py`

##### Adaptive Chunking

```python
from src.ml.rag_enhancements import AdaptiveChunker, ChunkingStrategy

chunker = AdaptiveChunker()

# Chunking adaptativo por tipo de documento
chunks = chunker.chunk_document(
    text=edital_text,
    document_id="doc-123",
    document_type="edital",  # ou "contrato", "lei"
    strategy=ChunkingStrategy.ADAPTIVE
)

# Cada chunk tem metadata enriquecida
for chunk in chunks:
    print(f"Chunk {chunk.metadata.chunk_index}")
    print(f"  Section: {chunk.metadata.section}")
    print(f"  Has values: {chunk.metadata.has_values}")
    print(f"  Completeness: {chunk.metadata.completeness_score}")
    print(f"  Topics: {chunk.metadata.main_topics}")
```

##### Query Expansion

```python
from src.ml.rag_enhancements import QueryExpander, QueryExpansionMethod

expander = QueryExpander()

# Expandir query com sinÃ´nimos e termos relacionados
expanded_queries = expander.expand_query(
    query="Prazo de licitaÃ§Ã£o",
    methods=[
        QueryExpansionMethod.SYNONYMS,
        QueryExpansionMethod.LEGAL_TERMS,
        QueryExpansionMethod.ACRONYMS,
    ]
)

# Resultado:
# [
#   "Prazo de licitaÃ§Ã£o",
#   "Prazo de certame",
#   "Prazo de processo licitatÃ³rio",
#   "PerÃ­odo de licitaÃ§Ã£o",
# ]
```

##### Semantic Deduplication

```python
from src.ml.rag_enhancements import SemanticDeduplicator

dedup = SemanticDeduplicator(similarity_threshold=0.95)

# Remover chunks duplicados
unique_chunks, removed_indices = dedup.deduplicate(
    chunks=all_chunks,
    embeddings=all_embeddings
)

print(f"Removed {len(removed_indices)} duplicate chunks")
print(f"Kept {len(unique_chunks)} unique chunks")
```

##### Citation Quality Scoring

```python
from src.ml.rag_enhancements import CitationQualityScorer

scorer = CitationQualityScorer()

# Avaliar qualidade da citaÃ§Ã£o
scores = scorer.score_citation(
    citation_text=retrieved_chunk.text,
    query=user_query,
    metadata=retrieved_chunk.metadata
)

print(f"Relevance: {scores['relevance']:.2f}")
print(f"Completeness: {scores['completeness']:.2f}")
print(f"Specificity: {scores['specificity']:.2f}")
print(f"Verifiability: {scores['verifiability']:.2f}")
print(f"Total Score: {scores['total']:.2f}")
```

---

## ğŸ” RAG (Retrieval-Augmented Generation)

### ConfiguraÃ§Ã£o Atual

**Arquivo**: `services/analyzer/src/config_rag.py`

```python
RAG_CONFIG = {
    'model': 'gemini-2.0-flash-001',
    'temperature': 0.2,
    'chunk_size': 512,
    'chunk_overlap': 100,
    'embedding_model': 'text-embedding-004',
    'similarity_top_k': 10,
    'vector_threshold': 0.5,
}
```

### EstratÃ©gias de Retrieval

#### 1. Standard (PadrÃ£o)

```python
retrieval_strategy = RetrievalStrategy.STANDARD
```

- Top-K similarity search
- Threshold de similaridade: 0.5
- RÃ¡pido e eficiente

#### 2. Reranked (ReordenaÃ§Ã£o)

```python
retrieval_strategy = RetrievalStrategy.RERANKED
```

- Retrieve top-K * 2
- Reranking com modelo cross-encoder
- Maior qualidade, mais lento

#### 3. Hybrid (HÃ­brido)

```python
retrieval_strategy = RetrievalStrategy.HYBRID
```

- Combine semantic + keyword search
- BM25 + Vector similarity
- Melhor recall

#### 4. MMR (Maximum Marginal Relevance)

```python
retrieval_strategy = RetrievalStrategy.MMR
```

- Diversidade de resultados
- Evita redundÃ¢ncia
- Melhor cobertura de tÃ³picos

### Pipeline RAG Completo

```python
from src.services.rag_service import RAGService
from src.ml.rag_enhancements import (
    AdaptiveChunker,
    QueryExpander,
    SemanticDeduplicator,
    CitationQualityScorer
)

# 1. Chunking Inteligente
chunker = AdaptiveChunker()
chunks = chunker.chunk_document(
    text=document.content,
    document_id=document.id,
    document_type="edital",
    strategy=ChunkingStrategy.ADAPTIVE
)

# 2. DeduplicaÃ§Ã£o
dedup = SemanticDeduplicator(similarity_threshold=0.95)
unique_chunks, _ = dedup.deduplicate(chunks, embeddings)

# 3. Importar para RAG
rag = RAGService(project_id=PROJECT_ID)
await rag.import_documents(corpus_id, unique_chunks)

# 4. Query com expansÃ£o
expander = QueryExpander()
expanded_queries = expander.expand_query(user_query)

# 5. Retrieve com melhor query
results = await rag.query(
    corpus_id=corpus_id,
    query=expanded_queries[0],  # Usar query expandida
    top_k=10
)

# 6. Score de qualidade das citaÃ§Ãµes
scorer = CitationQualityScorer()
for result in results:
    scores = scorer.score_citation(
        citation_text=result.text,
        query=user_query,
        metadata=result.metadata
    )
    result.quality_score = scores['total']

# 7. Filtrar por qualidade
high_quality_results = [
    r for r in results
    if r.quality_score >= 0.6
]
```

---

## ğŸ§ª A/B Testing de Modelos

### Conceitos

**A/B Testing** permite comparar diferentes modelos, configuraÃ§Ãµes e estratÃ©gias
de forma cientÃ­fica com trÃ¡fego real.

### Criando Experimentos

#### Via API

```bash
curl -X POST "http://localhost:8080/api/experiments/" \
  -H "Content-Type: application/json" \
  -d '{
    "experiment_id": "exp-gemini-pro-2024",
    "name": "Gemini Pro - High Quality",
    "description": "Testing Gemini Pro with reranking for higher quality",
    "model_variant": "gemini-1.5-pro-002",
    "temperature": 0.15,
    "retrieval_strategy": "reranked",
    "enable_reranking": true,
    "similarity_top_k": 15,
    "traffic_percentage": 0.2
  }'
```

#### Via Python

```python
from src.api.experiments import ab_test_manager

experiment = ab_test_manager.create_experiment(
    experiment_id="exp-gemini-pro-2024",
    name="Gemini Pro - High Quality",
    description="Testing Gemini Pro with reranking",
    model_variant=ModelVariant.GEMINI_PRO,
    temperature=0.15,
    retrieval_strategy=RetrievalStrategy.RERANKED,
    enable_reranking=True,
    similarity_top_k=15,
    traffic_percentage=0.2,  # 20% do trÃ¡fego
    is_active=True
)
```

### DistribuiÃ§Ã£o de TrÃ¡fego

```python
# Configurar mÃºltiplos experimentos
experiments = [
    ("control-flash", 0.5),      # 50% - Controle
    ("exp-pro-reranked", 0.3),   # 30% - Gemini Pro
    ("exp-flash-structured", 0.2) # 20% - Flash Estruturado
]

# Total deve somar 1.0 (100%)
```

### SeleÃ§Ã£o de Variantes

```python
# SeleÃ§Ã£o aleatÃ³ria (novo usuÃ¡rio)
variant = ab_test_manager.select_variant()

# SeleÃ§Ã£o consistente (mesmo usuÃ¡rio sempre vÃª mesma variante)
variant = ab_test_manager.select_variant(user_id="user-123")
```

### Coleta de MÃ©tricas

```python
import time

# InÃ­cio da anÃ¡lise
start_time = time.time()

# Executar anÃ¡lise com variante selecionada
result = await analyze_document(
    document=doc,
    model_variant=variant.model_variant,
    temperature=variant.temperature,
    retrieval_strategy=variant.retrieval_strategy
)

# Fim da anÃ¡lise
latency_ms = (time.time() - start_time) * 1000

# Registrar resultado
ab_test_manager.record_result(
    experiment_id=variant.experiment_id,
    latency_ms=latency_ms,
    tokens_used=result.tokens_used,
    success=result.success
)
```

### AnÃ¡lise de Resultados

```python
# Comparar dois experimentos
comparison = ab_test_manager.compare_experiments(
    "control-flash",
    "exp-pro-reranked"
)

print("=== Comparison ===")
print(f"Control: {comparison['experiment_a']['name']}")
print(f"  Success Rate: {comparison['experiment_a']['success_rate']:.1f}%")
print(f"  Avg Latency: {comparison['experiment_a']['avg_latency_ms']:.0f}ms")
print(f"  Feedback Score: {comparison['experiment_a']['feedback_score']:.1f}%")

print(f"\nVariant: {comparison['experiment_b']['name']}")
print(f"  Success Rate: {comparison['experiment_b']['success_rate']:.1f}%")
print(f"  Avg Latency: {comparison['experiment_b']['avg_latency_ms']:.0f}ms")
print(f"  Feedback Score: {comparison['experiment_b']['feedback_score']:.1f}%")

print(f"\nğŸ† Winner: {comparison['comparison']['winner']}")
print(f"Success Rate Diff: {comparison['comparison']['success_rate_diff']:+.1f}%")
print(f"Latency Diff: {comparison['comparison']['latency_diff_ms']:+.0f}ms")
print(f"Feedback Diff: {comparison['comparison']['feedback_diff']:+.1f}%")
```

### CritÃ©rios para Determinar Vencedor

O algoritmo usa **weighted scoring**:

- **Feedback Score**: 50% (mais importante)
- **Success Rate**: 30%
- **Latency**: 20% (menor Ã© melhor)

```python
def _determine_winner(exp_a, exp_b):
    # Normalizar mÃ©tricas
    feedback_a = exp_a.get_feedback_score() / 100
    feedback_b = exp_b.get_feedback_score() / 100

    success_a = exp_a.get_success_rate() / 100
    success_b = exp_b.get_success_rate() / 100

    # Latency (inverter - menor Ã© melhor)
    max_latency = max(exp_a.avg_latency_ms, exp_b.avg_latency_ms)
    latency_a = 1 - (exp_a.avg_latency_ms / max_latency)
    latency_b = 1 - (exp_b.avg_latency_ms / max_latency)

    # Score ponderado
    score_a = (feedback_a * 0.5) + (success_a * 0.3) + (latency_a * 0.2)
    score_b = (feedback_b * 0.5) + (success_b * 0.3) + (latency_b * 0.2)

    return "a" if score_a > score_b else "b"
```

### Exportando Resultados

```python
# Exportar para JSON
ab_test_manager.export_results("experiments_2024-01.json")
```

```json
{
  "experiments": [
    {
      "experiment_id": "control-flash",
      "name": "Control: Gemini 2.0 Flash",
      "metrics": {
        "total_requests": 1500,
        "success_rate": 94.5,
        "avg_latency_ms": 1250,
        "feedback_score": 78.3
      }
    },
    {
      "experiment_id": "exp-pro-reranked",
      "name": "Gemini Pro + Reranking",
      "metrics": {
        "total_requests": 450,
        "success_rate": 96.2,
        "avg_latency_ms": 2100,
        "feedback_score": 85.7
      }
    }
  ],
  "timestamp": "2024-01-15T10:30:00Z"
}
```

---

## ğŸ“Š Analytics e MÃ©tricas

### MÃ©tricas DisponÃ­veis

#### 1. System Health

```bash
GET /api/analytics/health
```

```json
{
  "status": "healthy",
  "uptime_seconds": 3600,
  "total_requests": 1250,
  "error_rate": 2.3,
  "avg_latency_ms": 1150,
  "p95_latency_ms": 2300,
  "p99_latency_ms": 4500
}
```

#### 2. Analytics Overview

```bash
GET /api/analytics/overview?period=24h
```

```json
{
  "period": "24h",
  "total_requests": 5420,
  "total_analyses": 1250,
  "total_errors": 125,
  "error_rate": 2.3,
  "avg_latency_ms": 1150,
  "p95_latency_ms": 2300,
  "avg_tokens_per_analysis": 850,
  "total_feedback": 450,
  "positive_feedback_rate": 78.5
}
```

#### 3. Time Series

```bash
GET /api/analytics/timeseries/latency?period=24h&granularity=1h
```

```json
[
  {
    "timestamp": "2024-01-15 10:00",
    "value": 1150.5,
    "label": "Avg: 1150.5ms"
  },
  {
    "timestamp": "2024-01-15 11:00",
    "value": 1220.3,
    "label": "Avg: 1220.3ms"
  }
]
```

#### 4. Model Performance

```bash
GET /api/analytics/models/performance?period=7d
```

```json
[
  {
    "model_variant": "gemini-2.0-flash-001",
    "total_requests": 8500,
    "avg_latency_ms": 1150,
    "p95_latency_ms": 2200,
    "success_rate": 94.5,
    "avg_tokens": 820,
    "feedback_score": 78.3
  },
  {
    "model_variant": "gemini-1.5-pro-002",
    "total_requests": 1500,
    "avg_latency_ms": 2100,
    "p95_latency_ms": 3800,
    "success_rate": 96.2,
    "avg_tokens": 950,
    "feedback_score": 85.7
  }
]
```

### Registrando MÃ©tricas Customizadas

```python
from src.api.analytics import metrics_store

# Request metrics
metrics_store.record_request(
    endpoint="/analyze",
    method="POST",
    status_code=200,
    latency_ms=1250.5,
    experiment_id="exp-pro-reranked"
)

# Analysis metrics
metrics_store.record_analysis(
    document_id="doc-123",
    analysis_type="full",
    duration_ms=12500,
    tokens_used=850,
    model_variant="gemini-2.0-flash-001",
    success=True,
    experiment_id="control-flash"
)

# Error metrics
metrics_store.record_error(
    error_type="ValidationError",
    error_message="Invalid document format",
    endpoint="/analyze",
    experiment_id="control-flash"
)
```

### Dashboards

Os dados de analytics podem ser visualizados em:

1. **Built-in Dashboard**: `/api/analytics/*` endpoints
2. **Cloud Monitoring**: Google Cloud Console
3. **Custom Dashboards**: Grafana/Looker (integraÃ§Ã£o futura)

---

## ğŸ”„ User Feedback Loop

### Tipos de Feedback

#### 1. Thumbs (ğŸ‘ ğŸ‘)

```python
from src.api.feedback import FeedbackCreate, FeedbackType

feedback = FeedbackCreate(
    document_id="doc-123",
    feedback_type=FeedbackType.THUMBS,
    is_positive=True,
    experiment_id="control-flash"
)
```

#### 2. Rating (â­ 1-5)

```python
feedback = FeedbackCreate(
    document_id="doc-123",
    feedback_type=FeedbackType.RATING,
    rating=4,
    experiment_id="exp-pro-reranked"
)
```

#### 3. Detailed Comment

```python
feedback = FeedbackCreate(
    document_id="doc-123",
    feedback_type=FeedbackType.DETAILED,
    category=FeedbackCategory.ACCURACY,
    comment="A anÃ¡lise foi precisa, mas faltou detalhar os prazos.",
    rating=4,
    experiment_id="control-flash"
)
```

#### 4. Correction

```python
feedback = FeedbackCreate(
    document_id="doc-123",
    feedback_type=FeedbackType.CORRECTION,
    category=FeedbackCategory.ACCURACY,
    comment="O prazo correto Ã© 30 dias, nÃ£o 45 dias.",
    correction="Prazo: 30 dias corridos a partir da publicaÃ§Ã£o.",
    is_positive=False,
    experiment_id="control-flash"
)
```

### API Endpoints

#### Criar Feedback

```bash
POST /api/feedback/

{
  "document_id": "doc-123",
  "feedback_type": "detailed",
  "category": "accuracy",
  "rating": 4,
  "comment": "Boa anÃ¡lise, mas poderia detalhar mais os prazos",
  "experiment_id": "control-flash"
}
```

#### Obter Feedbacks de um Documento

```bash
GET /api/feedback/document/doc-123
```

#### SumÃ¡rio EstatÃ­stico

```bash
GET /api/feedback/summary/stats?experiment_id=control-flash
```

```json
{
  "total_feedbacks": 450,
  "positive_count": 353,
  "negative_count": 75,
  "neutral_count": 22,
  "avg_rating": 4.2,
  "by_category": {
    "accuracy": 180,
    "completeness": 120,
    "clarity": 90,
    "relevance": 60
  },
  "by_type": {
    "thumbs": 250,
    "rating": 120,
    "detailed": 80
  }
}
```

#### Insights de Feedback

```bash
GET /api/feedback/insights/analyze?limit=500
```

```json
{
  "top_issues": [
    {
      "category": "completeness",
      "count": 45,
      "examples": [
        "Faltou analisar os anexos",
        "NÃ£o mencionou critÃ©rios de julgamento"
      ]
    },
    {
      "category": "accuracy",
      "count": 32,
      "examples": [
        "Prazo incorreto",
        "Valor estimado diferente do edital"
      ]
    }
  ],
  "improvement_suggestions": [
    "Adicionar anÃ¡lise automÃ¡tica de anexos",
    "Melhorar extraÃ§Ã£o de prazos",
    "Validar valores com mÃºltiplas fontes"
  ],
  "sentiment_trend": "improving",
  "critical_feedback_count": 12
}
```

### Ciclo de Melhoria

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User Interaction                                     â”‚
â”‚    â””â”€> Analisa documento com modelo/experimento         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Collect Feedback                                     â”‚
â”‚    â””â”€> Thumbs, Rating, Comments, Corrections            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Store & Analyze                                      â”‚
â”‚    â””â”€> FeedbackStore + MetricsStore                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Generate Insights                                    â”‚
â”‚    â””â”€> Top issues, Trends, Critical feedback            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Improve Models                                       â”‚
â”‚    â””â”€> Adjust configs, Create new experiments           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. A/B Test Improvements                                â”‚
â”‚    â””â”€> Compare old vs new                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Best Practices

### 1. RAG Best Practices

#### Chunking

```python
# âœ… BOM: Adaptive chunking por tipo
chunks = chunker.chunk_document(
    text=text,
    document_type="edital",  # Ajusta tamanho e estratÃ©gia
    strategy=ChunkingStrategy.ADAPTIVE
)

# âŒ RUIM: Tamanho fixo para todos os documentos
chunks = text.split('\n\n')  # Muito simplista
```

#### Query

```python
# âœ… BOM: Query expansion
expanded = expander.expand_query(query)
results = await rag.query(corpus_id, expanded[0])

# âŒ RUIM: Query literal sem expansÃ£o
results = await rag.query(corpus_id, query)
```

#### Deduplication

```python
# âœ… BOM: Semantic deduplication
unique_chunks, _ = dedup.deduplicate(chunks, embeddings)

# âŒ RUIM: Sem deduplicaÃ§Ã£o (aumenta custo e ruÃ­do)
all_chunks = chunks  # Pode ter duplicatas
```

### 2. A/B Testing Best Practices

#### Traffic Distribution

```python
# âœ… BOM: Gradual rollout
experiments = [
    ("control", 0.7),      # 70% controle
    ("new-variant", 0.3),  # 30% novo
]

# âŒ RUIM: 50/50 sem validaÃ§Ã£o
experiments = [
    ("control", 0.5),
    ("untested-variant", 0.5),  # Muito risco
]
```

#### Sample Size

```python
# âœ… BOM: Aguardar sample size adequado
if experiment.total_requests >= 100:
    # Comparar resultados
    comparison = manager.compare_experiments(...)

# âŒ RUIM: Decidir com pouco dados
if experiment.total_requests >= 10:  # Muito cedo!
    comparison = manager.compare_experiments(...)
```

#### Metrics

```python
# âœ… BOM: MÃºltiplas mÃ©tricas
- Feedback score (qualidade percebida)
- Success rate (funcionamento)
- Latency (performance)
- Token usage (custo)

# âŒ RUIM: Apenas uma mÃ©trica
- Latency  # Ignora qualidade!
```

### 3. Feedback Loop Best Practices

#### Collection

```python
# âœ… BOM: MÃºltiplos tipos de feedback
- Thumbs (rÃ¡pido)
- Rating (quantitativo)
- Comments (qualitativo)
- Corrections (ground truth)

# âŒ RUIM: Apenas thumbs
- Falta contexto para melhorias
```

#### Analysis

```python
# âœ… BOM: AnÃ¡lise regular de insights
insights = await get_feedback_insights(limit=500)
if insights.critical_feedback_count > 10:
    # Investigar problemas crÃ­ticos
    pass

# âŒ RUIM: Coletar mas nÃ£o analisar
# Feedback Ã© ignorado
```

### 4. Performance Best Practices

#### Caching

```python
# âœ… BOM: Cache de embeddings
@cache_embeddings(ttl=3600)
def get_document_embedding(text):
    return embedding_model.embed(text)

# âŒ RUIM: Recalcular sempre
def get_document_embedding(text):
    return embedding_model.embed(text)  # Caro!
```

#### Batching

```python
# âœ… BOM: Batch processing
embeddings = embedding_model.embed_batch(texts)

# âŒ RUIM: One-by-one
embeddings = [embedding_model.embed(t) for t in texts]
```

---

## ğŸ”§ Troubleshooting

### Problema: RAG retorna resultados irrelevantes

**Sintomas**:
- CitaÃ§Ãµes nÃ£o respondem a pergunta
- Score de qualidade baixo

**SoluÃ§Ãµes**:

1. **Aumentar threshold de similaridade**:
```python
vector_threshold = 0.7  # de 0.5 para 0.7
```

2. **Usar query expansion**:
```python
expanded = expander.expand_query(query)
```

3. **Verificar chunking**:
```python
# Chunks muito grandes ou pequenos?
chunk_size = 700  # Ajustar
```

### Problema: Experimento sem dados suficientes

**Sintomas**:
- `total_requests < 100`
- MÃ©tricas instÃ¡veis

**SoluÃ§Ãµes**:

1. **Aumentar traffic percentage**:
```python
experiment.traffic_percentage = 0.5  # de 0.2 para 0.5
```

2. **Aguardar mais tempo**:
```python
# Esperar pelo menos 1 semana com trÃ¡fego real
```

3. **Synthetic testing**:
```python
# Gerar trÃ¡fego de teste para validar funcionamento
```

### Problema: Feedback muito negativo

**Sintomas**:
- `feedback_score < 60%`
- Muitos comentÃ¡rios negativos

**SoluÃ§Ãµes**:

1. **Analisar insights**:
```python
insights = await get_feedback_insights()
print(insights.top_issues)
```

2. **Verificar experiment config**:
```python
# Temperature muito alta?
experiment.temperature = 0.1  # Reduzir
```

3. **Rollback se crÃ­tico**:
```python
experiment.is_active = False
experiment.traffic_percentage = 0.0
```

### Problema: LatÃªncia alta

**Sintomas**:
- `avg_latency_ms > 5000`
- Timeouts

**SoluÃ§Ãµes**:

1. **Reduzir top_k**:
```python
similarity_top_k = 5  # de 10 para 5
```

2. **Desabilitar reranking**:
```python
enable_reranking = False
```

3. **Usar modelo mais rÃ¡pido**:
```python
model_variant = ModelVariant.GEMINI_2_FLASH  # em vez de PRO
```

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o

- [Vertex AI RAG](https://cloud.google.com/vertex-ai/docs/vector-search/overview)
- [Gemini Models](https://ai.google.dev/models/gemini)
- [A/B Testing Guide](https://www.optimizely.com/optimization-glossary/ab-testing/)

### Arquivos Relacionados

```
services/analyzer/src/
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ ab_testing.py           # A/B testing framework
â”‚   â””â”€â”€ rag_enhancements.py     # RAG improvements
â”œâ”€â”€ services/
â”‚   â””â”€â”€ rag_service.py          # RAG core service
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ experiments.py          # Experiments API
â”‚   â”œâ”€â”€ analytics.py            # Analytics API
â”‚   â””â”€â”€ feedback.py             # Feedback API
â””â”€â”€ config_rag.py               # RAG configuration
```

### Scripts Ãšteis

```bash
# Criar experimento padrÃ£o
python -m src.ml.ab_testing

# Exportar resultados
curl -X POST "http://localhost:8080/api/experiments/export?filepath=results.json"

# Ver analytics
curl "http://localhost:8080/api/analytics/overview?period=7d"

# Insights de feedback
curl "http://localhost:8080/api/feedback/insights/analyze"
```

---

**Ãšltima atualizaÃ§Ã£o**: 22/11/2025
**VersÃ£o**: 1.0.0
**Mantido por**: AI/ML Team
